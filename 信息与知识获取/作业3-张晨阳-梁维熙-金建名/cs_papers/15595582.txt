Title: github.com/DataBiosphere/terra-examples/notebook_workflow

Authors: Nicole Deflaux

Keywords: 

Publication date: 2022-08-30

Description:
<h1>Programmatic execution of notebooks</h1>
In general, we run Jupyter notebooks <strong>interactively</strong>, but sometimes its useful to run them <strong>programmatically</strong>. Some use cases include:


A researcher might want to ensure their notebooks run with a known, clean virtual machine configuration without having to guess about the state of the machine they use for interactive analysis (e.g., use the workflow to test that the notebook has no unaccounted for dependencies on locally installed Python packages, R packages, or on local files).
A researcher might want to run a notebook with many different sets of parameters, all in parallel.
A researcher might have a long running notebook (e.g., taking hours or even days) that they wish to run on a machine separate from where they are working interactively.
A researcher might have a notebook that they want to run programmatically but do not wish to take the time to port it to a workflow.

<h2><a href="./notebook_workflow.wdl">notebook_workflow.wdl</a></h2>
Use WDL to programmatically execute a Jupyter notebook from start to finish. The notebook is executed on a new, clean
virtual machine (as opposed to where you normally run notebooks interactively).
This is useful not only for reproducibility and provenance, but to specifically confirm that the notebook
does not depend on any local dependencies (e.g., files or Python/R packages) installed where you normally
use Jupyter interactively.

This workflow will:


Optionally install a list of Python packages before executing the notebook.

This is because a kernel restart is often necessary to make use of Python packages installed during notebook execution time.
For R package dependencies, have the notebook install them at the beginning.


Optionally pass parameters to the notebook via <a href="https://papermill.readthedocs.io/">Papermill</a> to change its behavior.
Save the executed <code>ipynb</code> file (containing cell outputs) as a result of the workflow.
Also save an <code>html</code> copy of the executed ipynb file as a result of the workflow. This allows the notebook outputs to be read in the cloud console immediately.
If the notebook created any output files or directories underneath the current working directory, they will also be included in a tar output file.

Details and limitations:


If an error occurs during notebook execution, the resulting <code>ipynb</code> and <code>html</code> files are still saved, but
you will need to go look for them in the execution directory of the workflow run.
This workflow was originally written for <a href="https://app.terra.bio">app.terra.bio</a> and Google Cloud, but should run successfully on any Cromwell installation.
Environment variables <code>OWNER_EMAIL</code>, <code>WORKSPACE_BUCKET</code>, <code>WORKSPACE_NAME</code>, and <code>WORKSPACE_NAMESPACE</code> are not currently available, but this may change <a href="https://www.google.com/url?q=https://support.terra.bio/hc/en-us/community/posts/4411972716443-Make-workspace-environment-variables-available-in-workflow-configuration&amp;sa=D&amp;source=docs&amp;ust=1661812248047678&amp;usg=AOvVaw0jzAJVDbmwco9I4jFIu85L">in the future</a>. For now, if your notebook uses those, ensure that you can also inject the desired value via <a href="https://papermill.readthedocs.io/">Papermill</a> parameters.
It is not compatible with notebooks written to run on <a href="https://hail.is/">Hail</a> clusters.


Contents:
 - github.com-DataBiosphere-terra-examples-notebook_workflow_v1.0.1.zip
